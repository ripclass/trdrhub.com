name: 'Observability Tests'

on:
  pull_request:
    branches: [main, develop]
    paths:
      - 'dashboards/**'
      - 'canaries/**'
      - 'chaos/**'
      - 'slo_reporting/**'
      - 'security/**'
      - 'utils/**'
      - 'config/enterprise_config.yaml'
      - '.github/workflows/observability-tests.yml'
  push:
    branches: [main]
    paths:
      - 'dashboards/**'
      - 'canaries/**'
      - 'chaos/**'
      - 'slo_reporting/**'
      - 'security/**'
      - 'utils/**'
      - 'config/enterprise_config.yaml'
  schedule:
    - cron: '0 6 * * *'  # Daily at 6 AM UTC
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to test'
        required: true
        type: choice
        options:
          - staging
          - prod
          - both

env:
  PYTHON_VERSION: '3.11'
  AWS_REGION: 'eu-north-1'

jobs:
  config-validation:
    name: 'Validate Enterprise Config'
    runs-on: ubuntu-latest

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pyyaml jsonschema boto3

    - name: Validate YAML Syntax
      run: |
        echo "🔍 Validating enterprise configuration YAML syntax..."
        python -c "
        import yaml
        with open('config/enterprise_config.yaml', 'r') as f:
            config = yaml.safe_load(f)
        print('✅ YAML syntax is valid')
        "

    - name: Validate Configuration Structure
      run: |
        echo "🔍 Validating configuration structure..."
        python -c "
        import yaml

        with open('config/enterprise_config.yaml', 'r') as f:
            config = yaml.safe_load(f)

        # Required sections
        required_sections = ['environments', 'observability', 'chaos', 'slo', 'security']
        for section in required_sections:
            assert section in config, f'Missing required section: {section}'

        # Required environments
        required_envs = ['staging', 'prod']
        for env in required_envs:
            assert env in config['environments'], f'Missing environment: {env}'
            env_config = config['environments'][env]
            assert 'aws_region' in env_config, f'Missing aws_region for {env}'
            assert 'alarm_threshold' in env_config, f'Missing alarm_threshold for {env}'

        print('✅ Configuration structure is valid')
        "

    - name: Validate SLO Targets
      run: |
        echo "🔍 Validating SLO targets..."
        python -c "
        import yaml

        with open('config/enterprise_config.yaml', 'r') as f:
            config = yaml.safe_load(f)

        slo_config = config.get('slo', {})
        targets = slo_config.get('targets', {})

        for env in ['staging', 'prod']:
            assert env in targets, f'Missing SLO targets for {env}'
            env_targets = targets[env]

            required_targets = ['error_rate_per_minute', 'canary_success_rate', 'p95_latency_seconds', 'availability_percent']
            for target in required_targets:
                assert target in env_targets, f'Missing SLO target {target} for {env}'
                assert isinstance(env_targets[target], (int, float)), f'Invalid SLO target value for {target} in {env}'

        print('✅ SLO targets validation passed')
        "

  observability-syntax-check:
    name: 'Python Syntax Check'
    runs-on: ubuntu-latest

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pyyaml boto3 requests

    - name: Check Dashboard Manager
      run: |
        echo "🔍 Checking dashboard manager syntax..."
        python -m py_compile utils/dashboard_manager.py
        echo "✅ Dashboard manager syntax OK"

    - name: Check Canary System
      run: |
        echo "🔍 Checking canary system syntax..."
        python -m py_compile canaries/golden_path_canary.py
        echo "✅ Canary system syntax OK"

    - name: Check Chaos Controller
      run: |
        echo "🔍 Checking chaos controller syntax..."
        python -m py_compile chaos/chaos_controller.py
        echo "✅ Chaos controller syntax OK"

    - name: Check SLA Reporter
      run: |
        echo "🔍 Checking SLA reporter syntax..."
        python -m py_compile slo_reporting/sla_report_generator.py
        echo "✅ SLA reporter syntax OK"

    - name: Check Security Monitor
      run: |
        echo "🔍 Checking security monitor syntax..."
        python -m py_compile security/security_monitor.py
        echo "✅ Security monitor syntax OK"

    - name: Check Log Insights Manager
      run: |
        echo "🔍 Checking log insights manager syntax..."
        python -m py_compile utils/log_insights_manager.py
        echo "✅ Log insights manager syntax OK"

  dashboard-validation:
    name: 'Dashboard Configuration Validation'
    runs-on: ubuntu-latest

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pyyaml boto3

    - name: Validate Dashboard Template
      run: |
        echo "🔍 Validating dashboard template JSON..."
        python -c "
        import json

        with open('dashboards/lcopilot_health_template.json', 'r') as f:
            template = json.load(f)

        # Check required structure
        assert 'widgets' in template, 'Missing widgets array'
        widgets = template['widgets']
        assert isinstance(widgets, list), 'Widgets must be an array'
        assert len(widgets) > 0, 'Must have at least one widget'

        # Validate each widget
        for i, widget in enumerate(widgets):
            assert 'type' in widget, f'Widget {i} missing type'
            assert 'properties' in widget, f'Widget {i} missing properties'

            if widget['type'] == 'metric':
                props = widget['properties']
                assert 'metrics' in props, f'Metric widget {i} missing metrics'
                assert 'title' in props, f'Widget {i} missing title'

        print(f'✅ Dashboard template validation passed ({len(widgets)} widgets)')
        "

    - name: Test Dashboard Manager Dry Run
      run: |
        echo "🧪 Testing dashboard manager (dry run)..."
        python -c "
        import sys
        sys.path.append('utils')
        from dashboard_manager import DashboardManager

        # Test initialization
        manager = DashboardManager('staging')

        # Test configuration loading
        config = manager._load_config()
        assert 'environments' in config, 'Config missing environments'

        # Test template loading
        template = manager.load_dashboard_template()
        assert 'widgets' in template, 'Template missing widgets'

        print('✅ Dashboard manager dry run passed')
        "

  canary-validation:
    name: 'Canary System Validation'
    runs-on: ubuntu-latest

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pyyaml boto3 requests

    - name: Test Canary Initialization
      run: |
        echo "🧪 Testing canary initialization..."
        python -c "
        import sys
        sys.path.append('canaries')
        from golden_path_canary import GoldenPathCanary

        # Test initialization
        canary = GoldenPathCanary('staging')

        # Test configuration loading
        config = canary._load_config()
        assert 'environments' in config, 'Config missing environments'

        # Test scenarios loading
        scenarios = canary.scenarios
        assert len(scenarios) > 0, 'Must have at least one test scenario'

        for scenario in scenarios:
            assert 'name' in scenario, 'Scenario missing name'
            assert 'timeout' in scenario, 'Scenario missing timeout'

        print(f'✅ Canary initialization passed ({len(scenarios)} scenarios)')
        "

    - name: Test Canary Test File Generation
      run: |
        echo "🧪 Testing canary test file generation..."
        python -c "
        import sys, json
        sys.path.append('canaries')
        from golden_path_canary import GoldenPathCanary

        canary = GoldenPathCanary('staging')
        test_content = canary._generate_test_file_content()

        # Parse as JSON to validate
        test_data = json.loads(test_content.decode('utf-8'))
        assert test_data['canary_test'] == True, 'Test data missing canary flag'
        assert 'timestamp' in test_data, 'Test data missing timestamp'

        print('✅ Canary test file generation passed')
        "

  chaos-validation:
    name: 'Chaos System Validation'
    runs-on: ubuntu-latest

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pyyaml boto3

    - name: Test Chaos Controller Safety
      run: |
        echo "🧪 Testing chaos controller safety mechanisms..."
        python -c "
        import sys
        sys.path.append('chaos')
        from chaos_controller import ChaosController

        # Test staging environment (should be allowed)
        staging_controller = ChaosController('staging')
        is_allowed = staging_controller._check_environment_permissions()
        print(f'Staging chaos allowed: {is_allowed}')

        # Test production environment (should require force)
        prod_controller = ChaosController('prod')
        is_allowed = prod_controller._check_environment_permissions()
        print(f'Production chaos allowed: {is_allowed}')

        # Test validation
        valid, message = prod_controller.validate_experiment_request('error_spike', 120, force=False)
        assert not valid, 'Production should require --force flag'
        print(f'Production safety check: {message}')

        # Test fault type definitions
        fault_types = staging_controller.fault_types
        assert 'error_spike' in fault_types, 'Missing error_spike fault type'
        assert 'latency_injection' in fault_types, 'Missing latency_injection fault type'

        print(f'✅ Chaos controller safety validation passed ({len(fault_types)} fault types)')
        "

  slo-reporting-validation:
    name: 'SLO Reporting Validation'
    runs-on: ubuntu-latest

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pyyaml boto3

    - name: Test SLA Report Generator
      run: |
        echo "🧪 Testing SLA report generator..."
        python -c "
        import sys, json
        from datetime import datetime
        sys.path.append('slo_reporting')
        from sla_report_generator import SLAReportGenerator

        # Test initialization
        generator = SLAReportGenerator('staging')

        # Test configuration loading
        config = generator._load_config()
        assert 'slo' in config, 'Config missing SLO section'

        # Test SLO targets
        slo_targets = generator.slo_targets
        assert 'error_rate_per_minute' in slo_targets, 'Missing error rate SLO target'
        assert 'canary_success_rate' in slo_targets, 'Missing canary success SLO target'

        # Test HTML report generation (with mock data)
        mock_report_data = {
            'metadata': {
                'environment': 'staging',
                'report_month': '2024-01',
                'generated_at': datetime.now().isoformat()
            },
            'executive_summary': {
                'overall_slo_compliance': 95.0,
                'slos_met': 3,
                'total_slos': 4,
                'total_incidents': 2,
                'availability_percent': 99.5
            },
            'slo_compliance': {
                'metrics': [
                    {
                        'name': 'Test Metric',
                        'target': 5.0,
                        'actual': 3.2,
                        'compliance_percent': 95.0,
                        'unit': 'errors/min',
                        'status': 'met'
                    }
                ]
            },
            'recommendations': []
        }

        html_report = generator.generate_html_report(mock_report_data)
        assert '<html>' in html_report, 'HTML report generation failed'
        assert 'Test Metric' in html_report, 'HTML report missing test data'

        print('✅ SLA report generator validation passed')
        "

  security-monitoring-validation:
    name: 'Security Monitoring Validation'
    runs-on: ubuntu-latest

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pyyaml boto3

    - name: Test Security Monitor
      run: |
        echo "🧪 Testing security monitor..."
        python -c "
        import sys
        from datetime import datetime
        sys.path.append('security')
        from security_monitor import SecurityMonitor, SecurityEvent

        # Test initialization
        monitor = SecurityMonitor('staging')

        # Test configuration loading
        config = monitor._load_config()
        assert 'security' in config, 'Config missing security section'

        # Test risk score calculation
        risk_score = monitor._calculate_risk_score('192.168.1.100', 'US', 3)
        assert isinstance(risk_score, int), 'Risk score should be integer'
        assert 0 <= risk_score <= 100, 'Risk score should be 0-100'

        # Test suspicious IP detection
        is_suspicious = monitor._is_suspicious_ip('192.168.1.1')
        assert isinstance(is_suspicious, bool), 'Suspicious IP check should return boolean'

        # Test anomaly detection with mock events
        mock_events = [
            SecurityEvent(
                timestamp=datetime.now(),
                event_type='auth_failure',
                severity='medium',
                ip_address='10.0.0.1',
                user_agent='TestAgent',
                details={'attempts': 5},
                country_code='US',
                risk_score=60
            )
        ]

        anomalies = monitor.detect_anomalies(mock_events)
        assert isinstance(anomalies, list), 'Anomaly detection should return list'

        print(f'✅ Security monitor validation passed (risk score: {risk_score})')
        "

  integration-test-staging:
    name: 'Integration Test - Staging'
    runs-on: ubuntu-latest
    if: github.event_name != 'pull_request' && (github.ref == 'refs/heads/main' || github.event.inputs.environment == 'staging' || github.event.inputs.environment == 'both')
    environment: staging
    needs: [config-validation, observability-syntax-check]

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pyyaml boto3 requests

    - name: Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID_STAGING }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY_STAGING }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Test Dashboard Deployment (Dry Run)
      run: |
        echo "🧪 Testing dashboard deployment (dry run)..."
        python utils/dashboard_manager.py --env staging --verify || true
        echo "✅ Dashboard test completed"

    - name: Test Canary System
      run: |
        echo "🧪 Testing canary system (dry run)..."
        # Test canary initialization and configuration
        python -c "
        import sys
        sys.path.append('canaries')
        from golden_path_canary import GoldenPathCanary

        canary = GoldenPathCanary('staging')
        if canary.initialize_aws_client():
            print('✅ Canary AWS connection successful')
        else:
            print('⚠️ Canary AWS connection failed (expected in test environment)')
        "

    - name: Test Security Monitor
      run: |
        echo "🧪 Testing security monitor (dry run)..."
        python -c "
        import sys
        sys.path.append('security')
        from security_monitor import SecurityMonitor

        monitor = SecurityMonitor('staging')
        if monitor.initialize_aws_clients():
            print('✅ Security monitor AWS connection successful')
        else:
            print('⚠️ Security monitor AWS connection failed (expected in test environment)')
        "

  observability-summary:
    name: 'Observability Test Summary'
    runs-on: ubuntu-latest
    needs: [config-validation, observability-syntax-check, dashboard-validation, canary-validation, chaos-validation, slo-reporting-validation, security-monitoring-validation]
    if: always()

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Generate Test Summary
      run: |
        echo "## 📊 Observability Test Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Test Results" >> $GITHUB_STEP_SUMMARY
        echo "| Component | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|-----------|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| Config Validation | ${{ needs.config-validation.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Python Syntax Check | ${{ needs.observability-syntax-check.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Dashboard Validation | ${{ needs.dashboard-validation.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Canary Validation | ${{ needs.canary-validation.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Chaos Validation | ${{ needs.chaos-validation.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| SLO Reporting Validation | ${{ needs.slo-reporting-validation.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Security Monitoring Validation | ${{ needs.security-monitoring-validation.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Components Tested" >> $GITHUB_STEP_SUMMARY
        echo "- 🔧 **Dashboard Manager**: CloudWatch dashboard deployment and management" >> $GITHUB_STEP_SUMMARY
        echo "- 🔍 **Golden Path Canary**: Synthetic monitoring for upload→validate→report flow" >> $GITHUB_STEP_SUMMARY
        echo "- 🌪️ **Chaos Controller**: Controlled fault injection for resilience testing" >> $GITHUB_STEP_SUMMARY
        echo "- 📊 **SLA Report Generator**: Monthly SLO compliance reporting with PDF output" >> $GITHUB_STEP_SUMMARY
        echo "- 🔒 **Security Monitor**: Authentication anomaly detection and alerting" >> $GITHUB_STEP_SUMMARY
        echo "- 📋 **Log Insights Manager**: Advanced log analysis and cost optimization" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Next Steps" >> $GITHUB_STEP_SUMMARY
        echo "- 🚀 Deploy to staging environment using Terraform or CDK" >> $GITHUB_STEP_SUMMARY
        echo "- 📈 Monitor dashboards and metrics in CloudWatch" >> $GITHUB_STEP_SUMMARY
        echo "- 🧪 Run end-to-end canary tests to validate golden path" >> $GITHUB_STEP_SUMMARY
        echo "- 📊 Generate monthly SLA reports for compliance tracking" >> $GITHUB_STEP_SUMMARY

    - name: Check Overall Status
      run: |
        if [[ "${{ needs.config-validation.result }}" == "success" &&
              "${{ needs.observability-syntax-check.result }}" == "success" &&
              "${{ needs.dashboard-validation.result }}" == "success" &&
              "${{ needs.canary-validation.result }}" == "success" &&
              "${{ needs.chaos-validation.result }}" == "success" &&
              "${{ needs.slo-reporting-validation.result }}" == "success" &&
              "${{ needs.security-monitoring-validation.result }}" == "success" ]]; then
          echo "🎉 All observability tests passed!"
          echo "✅ Ready for deployment to staging environment"
        else
          echo "❌ Some observability tests failed"
          echo "🔍 Check the failed jobs above for details"
          exit 1
        fi